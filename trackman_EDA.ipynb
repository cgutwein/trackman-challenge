{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trackman Code Challenge\n",
    "## Exploratory Data Analysis\n",
    "We are going to extract the raw data for the challenge and assess at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the challenge description, we know that the data consists of a **series** of configuration files in `.json` format. Each file conatains a *SELECT*, *FROM*, and *WHERE* clause. From the .json we need to programmatically determine relationships between tables.\n",
    "\n",
    "#### Load Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open('./tables.tar.gz', \"r:gz\")\n",
    "for member in tar.getmembers():\n",
    "     f = tar.extractfile(member)\n",
    "     if f is not None:\n",
    "         content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "table_list = tar.getnames()\n",
    "print(type(table_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many files are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print(len(tar.getmembers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 38 files that we'll be interpreting. A sample of one of the queries is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using literal_eval to convert binary file to dictionary\n",
    "data = literal_eval(content.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'query': {'L': [{'M': {'group_by': {'S': 'a.org, s.game_id, g.game_reference, s.location_id, a.location, s.level_name, a.level, s.local_time_string, a.exception, a.access_start, a.access_end, s.game_state, s.home_team_name, a.rule_start_time, a.end_date, a.rule_end_time'}, 'where': {'S': \"a.season = 2019 and s.local_time_string like '2019%' and (s.local_time_string > a.rule_start_time or left(local_time_string, 10) = left(a.rule_start_time, 10)) and (s.local_time_string < a.rule_end_time or s.local_time_string = a.rule_end_time) and (s.local_time_string > a.access_start or a.access_start is null) and (s.local_time_string < a.access_end or a.access_end is null) and a.exception = false\"}, 'from': {'S': 'report.schedule_new_level s left join games.metadata g on s.game_id = g.game_id left join report.exp_access_rules_hist a on (a.level is null or a.level = s.level_name) and (a.location is null or s.location_id = a.location) and (a.team is null or s.home_team_id = a.team)'}, 'select': {'S': 'concat(a.org, s.game_id) as orggame, a.org, s.game_id, coalesce(a.location, s.location_id) as location_id, coalesce(a.level, s.level_name) as level, s.local_time_string, g.game_reference, a.access_start, a.access_end, a.exception, s.game_state, s.home_team_name, a.rule_start_time, a.end_date, a.rule_end_time, getdate() as date_processed'}}}]}, 'schema': {'S': 'report'}, 'table': {'S': 'scheduled_no_exceptions_hist'}}\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconstructing the Configuration File\n",
    "Now that we have the file in a workable format (i.e. python dictionary) we can explore how to find the relationships that we want to identify. First let's see what the *key* values are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['schema', 'table', 'query'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report\n"
     ]
    }
   ],
   "source": [
    "print(data['schema']['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduled_no_exceptions_hist\n"
     ]
    }
   ],
   "source": [
    "print(data['table']['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query\n",
    "The query is the item that will require some analysis. We'll need to further deconstruct the **query** value.  \n",
    "\n",
    "The query is made up of key-value pairs where the **key** is the SQL clause and a **value** is a data table or column.\n",
    "\n",
    "*Note: Using the *literal_eval* is possibly not the best way to decode our .json binary files. Something to re-visit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['from', 'group_by', 'where', 'select']) \n",
      "\n",
      "Sample Data SELECT String:  \n",
      "\n",
      "concat(a.org, s.game_id) as orggame, a.org, s.game_id, coalesce(a.location, s.location_id) as location_id, coalesce(a.level, s.level_name) as level, s.local_time_string, g.game_reference, a.access_start, a.access_end, a.exception, s.game_state, s.home_team_name, a.rule_start_time, a.end_date, a.rule_end_time, getdate() as date_processed \n",
      "\n",
      "Sample Data GROUP BY String:  \n",
      "\n",
      "a.org, s.game_id, g.game_reference, s.location_id, a.location, s.level_name, a.level, s.local_time_string, a.exception, a.access_start, a.access_end, s.game_state, s.home_team_name, a.rule_start_time, a.end_date, a.rule_end_time \n",
      "\n",
      "Sample Data WHERE String:  \n",
      "\n",
      "a.season = 2019 and s.local_time_string like '2019%' and (s.local_time_string > a.rule_start_time or left(local_time_string, 10) = left(a.rule_start_time, 10)) and (s.local_time_string < a.rule_end_time or s.local_time_string = a.rule_end_time) and (s.local_time_string > a.access_start or a.access_start is null) and (s.local_time_string < a.access_end or a.access_end is null) and a.exception = false \n",
      "\n",
      "Sample Data FROM String:  \n",
      "\n",
      "report.schedule_new_level s left join games.metadata g on s.game_id = g.game_id left join report.exp_access_rules_hist a on (a.level is null or a.level = s.level_name) and (a.location is null or s.location_id = a.location) and (a.team is null or s.home_team_id = a.team) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(data['query']['L'][0]))\n",
    "print(data['query']['L'][0]['M'].keys(), \"\\n\")\n",
    "print(\"Sample Data SELECT String: \", \"\\n\")\n",
    "print(data['query']['L'][0]['M']['select']['S'], \"\\n\")\n",
    "\n",
    "print(\"Sample Data GROUP BY String: \", \"\\n\")\n",
    "print(data['query']['L'][0]['M']['group_by']['S'], \"\\n\")\n",
    "\n",
    "print(\"Sample Data WHERE String: \", \"\\n\")\n",
    "print(data['query']['L'][0]['M']['where']['S'], \"\\n\")\n",
    "\n",
    "print(\"Sample Data FROM String: \", \"\\n\")\n",
    "print(data['query']['L'][0]['M']['from']['S'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are really only concerned with the **tables** and their **relationships** in this file. We want to write a function such that it will record the table and its dependencies for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_scrape(q):\n",
    "    \"\"\"\n",
    "    Determine the table dependencies from a FROM query.\n",
    "    \n",
    "    Input: q (str) - snippet of SQL query\n",
    "    Output: dependencies (list) - list of table dependencies\n",
    "    \n",
    "    utilizes re module\n",
    "    \"\"\"\n",
    "    ## Initialize list of dependencies\n",
    "    dependencies = []\n",
    "    words = q.split(' ') # split query on spaces\n",
    "    for word in words:\n",
    "        if '.' in word:\n",
    "            for t in table_list:\n",
    "                if word in t and word not in dependencies:\n",
    "                    dependencies.append(word)\n",
    "    return (dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['report.schedule_new_level', 'games.metadata', 'report.exp_access_rules_hist']\n"
     ]
    }
   ],
   "source": [
    "sample_from_statement = data['query']['L'][0]['M']['from']['S']\n",
    "print(table_scrape(sample_from_statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases\n",
    "A few test cases to ensure that the *table_scrape* function works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.games g inner join base.locations l on g.location_id = l.location_id left join rundown.location_history h on g.location_id = h.location_id AND g.date_created BETWEEN h.start_date AND h.end_date and g.application_type <> 3 AND g.game_reference NOT LIKE '%-BP-%'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rundown.location_history']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## games.metadata.json \n",
    "\n",
    "print(get_config_string('tables/games.metadata.json')['query']['L'][0]['M']['from']['S'])\n",
    "table_scrape(get_config_string('tables/games.metadata.json')['query']['L'][0]['M']['from']['S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rundown.locations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## rundown.location_history_rows.json\n",
    "\n",
    "print(get_config_string('tables/rundown.location_history_rows.json')['query']['M']['from']['S'])\n",
    "table_scrape(get_config_string('tables/rundown.location_history_rows.json')['query']['M']['from']['S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rundown.access_rules_end a full outer join broadcast.priority_lists p on a.season = p.season and (a.org = p.priority or a.org = p.org)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## report.exp_access_rules_hist\n",
    "\n",
    "print(get_config_string('tables/report.exp_access_rules_hist.json')['query']['L'][0]['M']['from']['S'])\n",
    "table_scrape(get_config_string('tables/report.exp_access_rules_hist.json')['query']['L'][0]['M']['from']['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate First Level Dependencies\n",
    "Now we need to build a reference dictionary for each table that contains a list of its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_first_level(table_list):\n",
    "    \"\"\"\n",
    "    Passes a list of tables (i.e. configuration files) and returns a dictionary with first-level dependencies.\n",
    "    \n",
    "    Input: table_list (list) - list of strings\n",
    "    Output: first_level_dict - dictionary of key (str) = table name and value (list) = list of dependencies.\n",
    "    \"\"\"\n",
    "    first_level_dict = dict() # initialize dictionary\n",
    "    for t in table_list:      # loop through each table\n",
    "        #print(t)\n",
    "        q = get_config_string(t) # load config file into memory\n",
    "        #print(q['query'])\n",
    "        try:\n",
    "            dep = table_scrape(q['query']['L'][0]['M']['from']['S'])\n",
    "        except:\n",
    "            dep = table_scrape(q['query']['M']['from']['S'])\n",
    "        first_level_dict[t] = dep\n",
    "    return (first_level_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(table_list[1:5])\n",
    "first_level_dependencies = populate_first_level(table_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(table_list)\n",
    "for t in table_list:\n",
    "    if 'base' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report.schedule_new_level\n",
      "games.metadata\n",
      "report.exp_access_rules_hist\n"
     ]
    }
   ],
   "source": [
    "words = sample_from_statement.split(' ')\n",
    "for word in words:\n",
    "    if '.' in word:\n",
    "        i = word.find(\".\")\n",
    "        if len(word[0:i]) > 2:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schema': {'S': 'rundown'}, 'table': {'S': 'location_history'}, 'query': {'M': {'from': {'S': 'rundown.location_history_rows l inner join rundown.location_history_rows r on l.location_id = r.location_id and ((l.row_num = r.row_num + 1 and l.combo_id <> r.combo_id) or (l.row_num = 1 and r.row_num = 1))'}, 'select': {'S': 'l.combo_id, l.location_id, l.oem_version, l.software_version, l.radar_serial, l.radar_model, l.radar_configuration, l.radar_measurement_mode, l.start_date, coalesce(lead(l.start_date) over (partition by l.location_id order by l.location_id, l.start_date),getdate()+1) as end_date, getdate() as date_processed'}}}}\n"
     ]
    }
   ],
   "source": [
    "print(get_config_string('rundown.location_history'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go further down the dependency tree, we'll need to extract configuration files by member name. Let's write this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_name = 'tables/report.schedule_new_level.json'\n",
    "temp_data = tar.extractfile(tar.getmember(config_file_name))\n",
    "temp_data = temp_data.read()\n",
    "temp_data = literal_eval(temp_data.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## writing a function to help getting data\n",
    "def get_config_string(p_file_name):\n",
    "    \"\"\"\n",
    "    Simple function to get .json string with associated configuration file name\n",
    "    \n",
    "    Input: p_file_name (str) - partial file name (table name)\n",
    "    Output: json_string (str) - file as a json string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #file_name = 'tables/' + p_file_name + '.json'\n",
    "        file_name = p_file_name\n",
    "        tar = tarfile.open('./tables.tar.gz', \"r:gz\")\n",
    "        json_string = tar.extractfile(tar.getmember(file_name))\n",
    "        json_string = json_string.read()\n",
    "        json_string = literal_eval(json_string.decode('utf8'))\n",
    "\n",
    "        return (json_string)\n",
    "    except:\n",
    "        return (None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schema': {'S': 'report'}, 'table': {'S': 'schedule_new_level'}, 'query': {'L': [{'M': {'from': {'S': 'lineup.schedule'}, 'where': {'S': \"left(local_time_string, 4) = '2019'\"}, 'select': {'S': \"id, game_id, foreignid, local_time, local_time_string, time_zone, game_no, location_id, location_name, league_id, league_name, case when level_name = 'D1' then 'NCAA' when level_name = 'KoreaBaseballOrganization' then 'KBO' when level_name = 'NPM' then 'NPB Minors' else level_name end as level_name, home_team_id, home_team_name, home_team_foreignid, away_team_id, away_team_name, away_team_foreignid, game_state, getdate() as date_processed\"}}}]}}\n"
     ]
    }
   ],
   "source": [
    "print(get_config_string('report.schedule_new_level'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing to Terminal\n",
    "We'll want an output to the terminal that is human readable. Here we'll write a function to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dependencies': [{'dependencies': [{'dependencies': [{'dependencies': [], 'table': 'lineup.schedule'}], 'table': 'report.schedule_new_level'}], 'table': 'report.schedule_new_level'}, {'dependencies': [{'dependencies': [{'dependencies': [], 'table': 'base.games'}, {'dependencies': [], 'table': 'base.locations'}, {'dependencies': [], 'table': 'rundown.location_history'}], 'table': 'games.metadata'}], 'table': 'games.metadata'}, {'dependencies': [{'dependencies': [{'dependencies': [], 'table': 'rundown.access_rules_end'}, {'dependencies': [], 'table': 'broadcast.priority_lists'}], 'table': 'report.exp_access_rules_hist'}], 'table': 'report.exp_access_rules_hist'}], 'table': 'report.scheduled_no_exceptions_hist'}\n"
     ]
    }
   ],
   "source": [
    "### print current output\n",
    "print(sample_output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report.scheduled_no_exceptions_hist \n",
      " \t|- report.schedule_new_level\n"
     ]
    }
   ],
   "source": [
    "print(sample_output_dict['table'], '\\n', '\\t|-', sample_output_dict['dependencies'][0]['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report.scheduled_no_exceptions_hist \n",
      "\n",
      "\t|\n",
      "\t|+ report.schedule_new_level\n",
      "\t|\n",
      "\t|+ games.metadata\n",
      "\t|\n",
      "\t|+ report.exp_access_rules_hist\n"
     ]
    }
   ],
   "source": [
    "print(sample_output_dict['table'], '\\n')\n",
    "for i in range(len(sample_output_dict['dependencies'])):\n",
    "    print('\\t|\\n\\t|+', sample_output_dict['dependencies'][i]['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dep_table(name, n=0):\n",
    "    \"\"\"\n",
    "    name (str) - table name\n",
    "    n (int) - number of levels\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        print(name, '\\n', file=open(\"output.txt\", \"a\"))\n",
    "    else:\n",
    "        print('\\t'*n, '|\\n', '\\t'*n, '|+', name, file=open(\"output.txt\", \"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report.delivered_combined_hist \n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = list(first_level_dependencies.keys())\n",
    "print_dep_table(names[0][7:-5], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_dep_table(t, dep_table, n=0):\n",
    "    print_dep_table(t[7:-5], n)\n",
    "    if len(first_level_dependencies[t]) > 0:\n",
    "        n += 1\n",
    "        for dep in dep_table[t]:\n",
    "            pretty_print_dep_table('tables/' + dep + '.json', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
